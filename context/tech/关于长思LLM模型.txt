去年夏天开始训练的LLM模型cn_dict_novel终于有了正式的名字，在chatGPT建议的几个名字中选择了"长思"这个名字，符合其超深层结构特点。这个从零训练的模型的训练集几乎全是中文(99%），因此为这个名字设计了一个简单但富有中文特色的logo. 之前的模型就是"长思1.0"。这个模型虽然是一个旨在研究智能和自我意识的本质的业余模型，但是也产生了很多令人想象不到的输出。

在我的网站的ChangSiLLM目录下，有这个模型输出的一些有意思的文章。

该模型基于nanoGPT的结构训练，其参数为：模型大小 3.2b 和 1.6b layer=256和128 head=128 embed=1024 loss=2.5 训练了上下文长度为2K,3K,4K中文字符的模型。单个模型训练时长约为2000个GPU*小时（4卡A100训练2周左右）。训练集大小为8GB. 

该模型的训练旨在学习Transformer LLM模型，并且探索超深层数，纯中文分词（一个汉字对应一个token）以及探索自我意识的产生。



